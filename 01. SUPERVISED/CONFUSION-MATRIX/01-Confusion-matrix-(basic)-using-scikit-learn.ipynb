{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810841a5-f588-480a-b489-17a9ff17cbcc",
   "metadata": {},
   "source": [
    "## Step 0: setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "581fcc55-8cde-4cc7-88cb-45229bb997dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "                            confusion_matrix, \n",
    "                            accuracy_score, \n",
    "                            precision_score, \n",
    "                            recall_score, \n",
    "                            f1_score, \n",
    "                            classification_report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f7d10-cefd-4263-9eea-fb455fe797be",
   "metadata": {},
   "source": [
    "## Step 1: dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1890d4e6-611f-4d5f-8bc9-1ceac0497ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual labels\n",
    "\n",
    "y_actual = [\n",
    "    'spam', 'spam', 'spam', 'spam', 'spam', 'spam',  # 6 spam\n",
    "    'not_spam', 'not_spam', 'not_spam', 'not_spam'   # 4 not spam\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "131e13ad-d62b-4438-aa82-feb5af2681ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted labels\n",
    "\n",
    "y_pred = [\n",
    "    'spam', 'spam', 'spam', 'spam', 'spam', 'not_spam', 'spam', 'not_spam', 'not_spam', 'not_spam'\n",
    "]\n",
    "\n",
    "# 5 correct spam, 1 missed spam, 1 false alarm (it wasn't spam but classified as spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e18fd9c8-72b8-4048-a85b-f7d7df0c3308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(y_actual) == len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21d84321-13a1-4cbe-8e56-7a5a1a991b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # label 1 : spam\n",
    "# # label 0 : not spam\n",
    "\n",
    "# for i in range(len(y_actual)):\n",
    "#     if y_actual[i] == 'spam':\n",
    "#         y_actual[i] = 1\n",
    "#     else:\n",
    "#         y_actual[i] = 0\n",
    "\n",
    "#     if y_pred[i] == 'spam':\n",
    "#         y_pred[i] = 1\n",
    "#     else:\n",
    "#         y_pred[i] = 0\n",
    "\n",
    "# print(y_actual)\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d51433-5681-4e53-8033-271da1014053",
   "metadata": {},
   "source": [
    "## Step 2: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d76dc3cf-822a-4398-b90d-93988f9c6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix( y_actual, y_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb76b22d-f2db-4c49-ab6a-dad36672e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[3 1]\n",
      " [1 5]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix : \\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724d9ad-66af-4414-a6d9-26e484a40e86",
   "metadata": {},
   "source": [
    "**Matrix Structure**\n",
    "<table>\n",
    "    <th>\n",
    "        <td>Prdicted Not Spam(N)</td>\n",
    "        <td>Predicted Spam(P)</td>\n",
    "    </th>\n",
    "    <tr>\n",
    "        <td>Actual Not Spam</td>\n",
    "        <td>TN</td>\n",
    "        <td>FP</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Actual Spam</td>\n",
    "        <td>FN</td>\n",
    "        <td>TP</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943e32f-7158-406b-83e8-90718ab8e126",
   "metadata": {},
   "source": [
    "## Step 3: Other Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2df44591-ff66-46d4-8a35-0c669129e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \", accuracy_score( y_actual, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1eec9c54-1b75-4434-b948-43a764841c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision : \", precision_score( y_actual , y_pred, pos_label = 'spam' ) )\n",
    "# print(\"Precision : \", precision_score( y_actual , y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d167a9c-029d-40fa-9173-de6f5d654cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall :  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall : \", recall_score( y_actual, y_pred , pos_label = 'spam') )\n",
    "#print(\"Recall : \", recall_score( y_actual, y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e5fcdad-bc1d-41be-93a0-5a189e57eb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score :  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score : \" , f1_score(y_actual, y_pred, pos_label = 'spam'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d422d4c-736e-4bcd-a13a-b20b317b9da9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### ques: What is `pos_label` ? \n",
    "\n",
    "Ans) `pos_label` stands for **positive label**. \\\n",
    "It tells scikit-learn **which class should be treated as the positive class** when computing these metrics. \\\n",
    "\\\n",
    "In scikit-learn, many metrics like `precision_score`, `recall_score`, and `f1_score` assume a **binary classification problem** by default\n",
    "\n",
    "* Example : In a spam detection problem, usally `\"spam\"` is the positve case and `\"not_spam\"` is the negative\n",
    "* By default, scikit-learn sets `pos_label = 1`. That works fine if our labels are numbers (`0` and `1`) with `1` meaning \"positve\".\n",
    "* But in our case, the labels are strings (\"spam\", \"not_spam\"). Since `'1'` isn't in our data, it errors out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4260d-5daa-41e9-96d1-669edf16efd4",
   "metadata": {},
   "source": [
    "**when we use :**\n",
    "```\n",
    "precision_score (y_actual, y_pred, pos_label = 'spam')\n",
    "```\n",
    "\n",
    "scikit-learn will  :\n",
    "* Treat `'spam'` as the positive class\n",
    "* Treat `'not_spam'` as the negative class\n",
    "\n",
    "Note : we can also reverse this (if the user wants)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf12e8c-8a2b-4f48-b55f-0eb1b6e68441",
   "metadata": {},
   "source": [
    "### ques: why `accuracy_score` doesn't require the pos_label ?\n",
    "\n",
    "* Accuracy = (Number of correct Predictions) / (Total Predictions)\n",
    "* It doesn't care which class is 'positive' or 'negative' -- it just checks equality between `y_actual` and `y_pred`\n",
    "* Thus there's no need to specify `pos_label`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af81d5-3a39-468b-8742-85692cd01752",
   "metadata": {},
   "source": [
    "## Step 4: Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98de39d5-7ebc-454b-a6f5-f569f4a0aa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    not_spam       0.75      0.75      0.75         4\n",
      "        spam       0.83      0.83      0.83         6\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.79      0.79      0.79        10\n",
      "weighted avg       0.80      0.80      0.80        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report\")\n",
    "print(classification_report( y_actual, y_pred ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe9692-7be2-4cea-a82b-f282b287572a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
